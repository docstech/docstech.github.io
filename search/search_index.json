{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Overview Kubernetes \u00b6 Kubernetes setup \u00b6 Guide To Deploy kubernetes master master using ansible \u00b6 Kubernetes cluster setup documentation here kubespray follow the instructions. Setup and basic config \u00b6 Install ansible Set ip host in inventory Select haproxy mode to loadbalancer default loadbalancer api server localhost (file all.yaml) Disable loadbalancer_apiserver_localhost (file all. yaml) Enable metrics (addons.yaml) Select the k8s version you want to deploy (addons.yaml) Enable oidc_auth (addons.yaml) Enable to deploy metal enabled remember set ip range for it (addons.yaml) Mod_proxy config to ipvs because I'm flowing metallB, so set the strict_arp = true (addons.yaml) Enable audit_log (k8s_cluster.yaml) View again, do you need false or true again.? otherwise we proceed to flow ansible (wherever you can connect to the k8s cluster) (k8s_cluster.yaml) Start ansible flow for example: ansible-playbook -b -v -i inventory/halome-cluster/hosts.yaml cluster.yml --private-key /root/hahalolo.key --become --become-user=root --user=root --flush-cache -e ansible_ssh_port =23465 K8s connect Storage \u00b6 Instructions to connect Storage k8s with ceph \u00b6 Follow the instructions to remember to change ip and ib link Traefik \u00b6 Installation instructions traefik \u00b6 Select vision and follow step-by-step instructions in the article Download the values.yaml file and edit it link download file values.yaml file and edit. Open port 9000 to access dashboard Add storage class to replica traefik To enable x-forwarded-for vs x-real-ip in traefik in values.yaml file navigate to and edit as below: service: spec: externalTrafficPolicy: Local (Add this line) Finally apply to deploy is done EFK \u00b6 EFK installation instructions \u00b6 Install elasticsearch by helm. Drag the file values.yaml to edit the section Storage helm inspect values \u200b\u200belastic/elasticsearch > elastic.yaml Install kibana by helm helm inspect values \u200b\u200belastic/kibana > kibana.yaml Install fluentbit with a helm. Drag the file values.yaml to edit. Enable deployment to node master : tolerations: - key: node-role.kubernetes.io/control-plane operator: Exists effect: NoSchedule - key: node-role.kubernetes.io/master operator: Exists effect: NoSchedule Add permission for fluentbit to read audit log of master node: extraVolumes: - name: audit hostPath: path: /etc/kubernetes/audit-policy/apiserver-audit-policy.yaml type: File - name: audit-log hostPath: path: /var/log/kubernetes/audit/ type: DirectoryOrCreate extraVolumeMounts: - mountPath: /etc/kubernetes/audit-policy/apiserver-audit-policy.yaml name: audit readOnly: true - mountPath: / var/log/kubernetes/audit/ name: audit-log readOnly: false Note: To read the log and output to elasticsearch need to be careful and configure the INPUT and OUTPUT subdivisions (remember to add in) out for audit log). Finally, the helm install each one is done. SSO kubernetes \u00b6 SSO kubernetes Deployment Guide \u00b6 First we need to install keycloak choose the download version and follow the steps here [link] (https://syncbricks.com/keycloak-installation-ubuntu-20-04-version-19-step-by-step/) . Once installed create ream, client and openid group Deploy public authentication openunison Inquire more files values.yaml note: secret file is keycloak secret client In kubernetes cluster we need to configure add kube- apiconfig.yaml as follows: - --oidc-issuer-url=https://k8sou.halome.dev/auth/idp/k8sIdp - --oidc-client-id=kubernetes - --oidc-ca -file=/etc/kubernetes/ssl/ca-ou.pem - --oidc-username-claim=sub - --oidc-groups-claim=groups - --oidc-username-prefix=- - --oidc- groups-prefix= - --oidc-ca-file we run the command to get the CA file out and put it in the specified directory kubectl get secret ou-tls-certificate -n openunison -o json | jq -r '.data[\"tls.crt\"]' | base64 -d > /path/to/ou-ca.pem Implement RBA for authorization and groups-based user rights in Keycloak Reference here . Note: In this entire tutorial, we use our ssl and set up more domains. After completing the above steps, we can login to get the token and view the dashboard . Bootkube \u00b6 Guide to install Bootkube to reduce deployment or delete deploy via discord channel \u00b6 Create channel on discord doc Need any configuration add see doc and follow instructions in doc","title":"Overview Kubernetes"},{"location":"#overview-kubernetes","text":"","title":"Overview Kubernetes"},{"location":"#kubernetes-setup","text":"","title":"Kubernetes setup"},{"location":"#guide-to-deploy-kubernetes-master-master-using-ansible","text":"Kubernetes cluster setup documentation here kubespray follow the instructions.","title":"Guide To Deploy kubernetes master master using ansible"},{"location":"#setup-and-basic-config","text":"Install ansible Set ip host in inventory Select haproxy mode to loadbalancer default loadbalancer api server localhost (file all.yaml) Disable loadbalancer_apiserver_localhost (file all. yaml) Enable metrics (addons.yaml) Select the k8s version you want to deploy (addons.yaml) Enable oidc_auth (addons.yaml) Enable to deploy metal enabled remember set ip range for it (addons.yaml) Mod_proxy config to ipvs because I'm flowing metallB, so set the strict_arp = true (addons.yaml) Enable audit_log (k8s_cluster.yaml) View again, do you need false or true again.? otherwise we proceed to flow ansible (wherever you can connect to the k8s cluster) (k8s_cluster.yaml) Start ansible flow for example: ansible-playbook -b -v -i inventory/halome-cluster/hosts.yaml cluster.yml --private-key /root/hahalolo.key --become --become-user=root --user=root --flush-cache -e ansible_ssh_port =23465","title":"Setup and basic config"},{"location":"#k8s-connect-storage","text":"","title":"K8s connect Storage"},{"location":"#instructions-to-connect-storage-k8s-with-ceph","text":"Follow the instructions to remember to change ip and ib link","title":"Instructions to connect Storage k8s with ceph"},{"location":"#traefik","text":"","title":"Traefik"},{"location":"#installation-instructions-traefik","text":"Select vision and follow step-by-step instructions in the article Download the values.yaml file and edit it link download file values.yaml file and edit. Open port 9000 to access dashboard Add storage class to replica traefik To enable x-forwarded-for vs x-real-ip in traefik in values.yaml file navigate to and edit as below: service: spec: externalTrafficPolicy: Local (Add this line) Finally apply to deploy is done","title":"Installation instructions traefik"},{"location":"#efk","text":"","title":"EFK"},{"location":"#efk-installation-instructions","text":"Install elasticsearch by helm. Drag the file values.yaml to edit the section Storage helm inspect values \u200b\u200belastic/elasticsearch > elastic.yaml Install kibana by helm helm inspect values \u200b\u200belastic/kibana > kibana.yaml Install fluentbit with a helm. Drag the file values.yaml to edit. Enable deployment to node master : tolerations: - key: node-role.kubernetes.io/control-plane operator: Exists effect: NoSchedule - key: node-role.kubernetes.io/master operator: Exists effect: NoSchedule Add permission for fluentbit to read audit log of master node: extraVolumes: - name: audit hostPath: path: /etc/kubernetes/audit-policy/apiserver-audit-policy.yaml type: File - name: audit-log hostPath: path: /var/log/kubernetes/audit/ type: DirectoryOrCreate extraVolumeMounts: - mountPath: /etc/kubernetes/audit-policy/apiserver-audit-policy.yaml name: audit readOnly: true - mountPath: / var/log/kubernetes/audit/ name: audit-log readOnly: false Note: To read the log and output to elasticsearch need to be careful and configure the INPUT and OUTPUT subdivisions (remember to add in) out for audit log). Finally, the helm install each one is done.","title":"EFK installation instructions"},{"location":"#sso-kubernetes","text":"","title":"SSO kubernetes"},{"location":"#sso-kubernetes-deployment-guide","text":"First we need to install keycloak choose the download version and follow the steps here [link] (https://syncbricks.com/keycloak-installation-ubuntu-20-04-version-19-step-by-step/) . Once installed create ream, client and openid group Deploy public authentication openunison Inquire more files values.yaml note: secret file is keycloak secret client In kubernetes cluster we need to configure add kube- apiconfig.yaml as follows: - --oidc-issuer-url=https://k8sou.halome.dev/auth/idp/k8sIdp - --oidc-client-id=kubernetes - --oidc-ca -file=/etc/kubernetes/ssl/ca-ou.pem - --oidc-username-claim=sub - --oidc-groups-claim=groups - --oidc-username-prefix=- - --oidc- groups-prefix= - --oidc-ca-file we run the command to get the CA file out and put it in the specified directory kubectl get secret ou-tls-certificate -n openunison -o json | jq -r '.data[\"tls.crt\"]' | base64 -d > /path/to/ou-ca.pem Implement RBA for authorization and groups-based user rights in Keycloak Reference here . Note: In this entire tutorial, we use our ssl and set up more domains. After completing the above steps, we can login to get the token and view the dashboard .","title":"SSO kubernetes Deployment Guide"},{"location":"#bootkube","text":"","title":"Bootkube"},{"location":"#guide-to-install-bootkube-to-reduce-deployment-or-delete-deploy-via-discord-channel","text":"Create channel on discord doc Need any configuration add see doc and follow instructions in doc","title":"Guide to install Bootkube to reduce deployment or delete deploy via discord channel"},{"location":"ceph-s3-intelligent-tiering-lifecycle-management/","text":"S3 Intelligent Tiering vs. Lifecycle Management \u00b6 Creating an intelligent object storage system with Ceph\u2019s Object Lifecycle Management Scope/Description \u00b6 This guide runs through in detail what lifecycle management is, how to deploy it, and how to create custom storage classes with RGW\u2019s. Prerequisites \u00b6 This guide assumes you have a Ceph cluster deployed with 2 RGW\u2019s in place and the required RGW pools created such as buckets.data, buckets.index, buckets.non-ec, rgw.log rgw.meta. If this is not the case, refer to the 45Drives Ceph ansible guide to deploy your RGW\u2019s or the official Ceph docs. Steps \u00b6 This guide will detail how we can move objects to different classes of storage in order to separate them based on time. For example, we can dictate that any object older than N number of days be moved to a storage class called \u201ccold\u201d. Instead of moving to a separate storage class, the objects can instead be set to expire after a period of time, which will then be sent to Ceph garbage collection for deletion. Rather than deleting, the secondary storage class could be tied to a pool with a much more aggressive erasure coding pool that gives much better storage efficiency, but may not be quite as redundant. This could be a good tradeoff for files that are no longer deemed necessary for production, but still want to keep a copy rather than deleting outright. Creation of new storage class and pool \u00b6 Note: If you only wish to create a rule that expires and deletes objects, rather than moving them to a new storage tier, this section can be skipped, and you can move directly to \u201cCreating S3 lifecycle management policies\u201d This guide assumes the default RGW configuration. If you have a custom zonegroup or zones and placement targets set up you will have to replace the default names for your configured names. To find out if you are using the default configuration, run the following commands: root@labosd1:~# radosgw-admin zonegroup list root@labosd1:~# radosgw-admin zone list root@labosd1:~# radosgw-admin zonegroup placement list For a default configuration, it will look like this: If the output is different, be sure to replace \u201cdefault\u201d and \u201cdefault-placement\u201d with the correct naming scheme in the guide that follows. Add a new storage class \u00b6 First, create a new storage class within the Ceph RGW structure so a new pool can be created and then assign that pool to the new class. This guide is going to use the name \u201cCOLD\u201d for its storage class. You may want to instead use a storage class that AWS uses. They have predefined storage classes such as 1A-Standard. you Can find them here root@labosd1:~# radosgw-admin zonegroup placement add \\\\ --rgw-zonegroup default \\\\ --placement-id default-placement \\\\ --storage-class COLD Next, provide the zone placement info for the storage class that was just created. Also, set the name of the pool that the storage class \u201cCOLD\u201d is going to be tied to. The pool does not have to be created yet. It can be done in the next step. root@labosd1:~# radosgw-admin zone placement add \\\\ --rgw-zone default \\\\ --placement-id default-placement \\\\ --storage-class COLD \\\\ --data-pool default.rgw.cold.data \\\\ --compression lz4 The output should show that under storage classes \u201cCOLD\u201d the new data pool is assigned to it. Next, create the new pool. This can be done via the dashboard for ease where you can select the pool type (replicated or erasure coded) the number of placement groups, the type of compression, etc. Just make sure the pool name matches the one used in the command above. The new storage class is now complete and ready to use. The next steps are to create and design a lifecycle to do exactly what you want it to do. This guide will use a few simple examples. Creating S3 Lifecycle management policies \u00b6 The next step will require s3cmd to be installed on the machine. You can also use a tool like S3 browser for Windows which allows you to create s3 ACL policies. This guide will use s3cmd. Install s3cmd and then configure it using their built-in tool using \u2013configure. Enter the access key and secret key for the s3 bucket the s3 policy will be built on, and point it to the proper RGW endpoint and port. The below example will provide formatting for settings. Ensure for \u201cDNS-style bucket+hostname:port template\u201d is simply the IP address of your RGW and the port it uses without the bucket or location vars used. Default region is not needed. You can simply press enter on default region and encryption. At the end of the config, s3cmd will ask to test the settings. If it succeeds, you can continue. S3cmd will generate a config file at /root/.s3cfg. root@octrgw1:~# apt install s3cmd root@octrgw1:~# s3cmd \u2013configure To test functionality, run s3cmd ls. This should list the buckets tied to the provided access key and secret key. Lifecycle configuration \u00b6 With the storage class created and s3cmd configured, we can now create some lifecycle management rules. These management rules are to be created in an xml file which can then be passed to the bucket with s3cmd. A more comprehensive list on lifecycle rules can be found here . However, please note this page is directed towards Amazon S3 and so remember some things may not carry over 1 to 1 with Ceph, although most of the rules should. With the lifecycle management tool, you have the option for \u201ctransition\u201d and \u201cexpiration\u201d You can create rules that have 2 steps that incorporates both. In order to create these lifecycle rules, create an XML file and create your rule within that XML file. For example, a rule can be made to move objects to a new storage class after 1 month, and then expire them to be deleted after 2. That lifecycle rule would look something like this: <LifecycleConfiguration> <Rule> <ID>Transition then Expire Rule</ID> <Filter> <Prefix></Prefix> </Filter> <Status>Enabled</Status> <Transition> <Days>30</Days> <StorageClass>COLD</StorageClass> </Transition> <Expiration> <Days>60</Days> </Expiration> </Rule> </LifecycleConfiguration> For a simpler rule, you can dictate simply to delete objects older than a set date. This rule would look similar to this: <LifecycleConfiguration> <Rule> <ID>Expire after 1 year</ID> <Filter> <Prefix></Prefix> </Filter> <Status>Enabled</Status> <Expiration> <Days>365</Days> </Expiration> </Rule> </LifecycleConfiguration> * Finally, for a rule to only move objects to a new storage class, and not mark them to be expired, you could use a rule like this: <LifecycleConfiguration> <Rule> <ID>Transition objects to COLD storage class</ID> <Filter> <Prefix></Prefix> </Filter> <Status>Enabled</Status> <Transition> <Days>30</Days> <StorageClass>COLD</StorageClass> </Transition> </Rule> </LifecycleConfiguration> * Inside these rules you will also find \u201cPrefix\u201d. This allows flexibility to have different rules running all on a single bucket and as long as the objects are uploaded with the correct prefix, the rule will only apply to it. This can allow you to have multiple expire rules within a single bucket. This would look something like this: <LifecycleConfiguration> <Rule> <ID>prefix expiration</ID> <Filter> <Prefix>dir1</Prefix> </Filter> <Status>Enabled</Status> <Expiration> <Days>120</Days> </Expiration> <Filter> <Prefix>dir2</Prefix> </Filter> <Status>Enabled</Status> <Expiration> <Days>60</Days> </Expiration> </Rule> </LifecycleConfiguration> * The above rule will expire all objects after 120 days with the prefix of dir1, and will expire all objects after 60 days with a prefix of dir2. * To put created rules in effect, create a new xml file on the same host s3cmd is installed. * Copy any rule above along with any changes needed for your environment. * Example: Save the file. This file is saved as lifecycle-expire.xml . The s3 bucket that this policy will be attached to is called lifecycletest. Using s3cmd, set the lifecycle policy on the correct bucket. root@octrgw1:~# s3cmd setlifecycle lifecycle-expire.xml s3://lifecycletest Test to see if the lifecycle has been set. root@octrgw1:~# s3cmd info s3://lifecycletest When using transition rules instead of expire rules, s3cmd info will not properly list it. You will need a tool like S3 browser to view the lifecycle rule. S3 browser can also be used to create these rules. The only downside of using S3 Browser is that it does not allow the use of custom storage classes such as the use of \u201cCOLD\u201d. It uses only standard Amazon S3 naming, so when using transition rather than expiration it is still better to use s3cmd and the XML files. A guide for s3 browser lifecycle management can be found here: https://s3browser.com/bucket-lifecycle-configuration.aspx Verification \u00b6 If you would like to create and test some of these rules, it is best to use Ceph\u2019s built in RGW debug mode. This allows a set period of time (typically 60 seconds) will correlate to 24 hours. This allows fast testing of rules to ensure they are doing exactly what is expected, and gives the ability to iterate quickly. To do this, you must SSH into your nodes acting as your RGW\u2019s. Once inside, open ceph.conf with sudo permissions. It can be found at /etc/ceph/ceph.conf Add the following line to the bottom of the conf file: rgw lc debug interval = 60 This sets the lifecycle interval to correlate every 60 seconds to 24 hours. Make sure if you are using multiple RGW\u2019s to add this to each RGW\u2019s config file. Once the config file is saved, restart the RGW\u2019s. Replace the name octrgw1 with the name of your RGW. root@octrgw1:~# systemctl restart ceph-radosgw@rgw.octrgw1 Troubleshooting =============== If you are having trouble with your lifecycle rules, it is most likely because the rules are written incorrectly, or the RGW debug interval is not working. In order to check the status of your RGW lifecycle rules, run the following command on a node with an admin keyring on it. root@octrgw1:~# radosgw-admin lc list This lists the lifecycle rules you have in place. If they are not set to \u201ccomplete\u201d you can force Ceph to start the rule by running the following command: root@octrgw1:~# radosgw-admin lc process","title":"S3 Intelligent Tiering vs. Lifecycle Management"},{"location":"ceph-s3-intelligent-tiering-lifecycle-management/#s3-intelligent-tiering-vs-lifecycle-management","text":"Creating an intelligent object storage system with Ceph\u2019s Object Lifecycle Management","title":"S3 Intelligent Tiering vs. Lifecycle Management"},{"location":"ceph-s3-intelligent-tiering-lifecycle-management/#scopedescription","text":"This guide runs through in detail what lifecycle management is, how to deploy it, and how to create custom storage classes with RGW\u2019s.","title":"Scope/Description"},{"location":"ceph-s3-intelligent-tiering-lifecycle-management/#prerequisites","text":"This guide assumes you have a Ceph cluster deployed with 2 RGW\u2019s in place and the required RGW pools created such as buckets.data, buckets.index, buckets.non-ec, rgw.log rgw.meta. If this is not the case, refer to the 45Drives Ceph ansible guide to deploy your RGW\u2019s or the official Ceph docs.","title":"Prerequisites"},{"location":"ceph-s3-intelligent-tiering-lifecycle-management/#steps","text":"This guide will detail how we can move objects to different classes of storage in order to separate them based on time. For example, we can dictate that any object older than N number of days be moved to a storage class called \u201ccold\u201d. Instead of moving to a separate storage class, the objects can instead be set to expire after a period of time, which will then be sent to Ceph garbage collection for deletion. Rather than deleting, the secondary storage class could be tied to a pool with a much more aggressive erasure coding pool that gives much better storage efficiency, but may not be quite as redundant. This could be a good tradeoff for files that are no longer deemed necessary for production, but still want to keep a copy rather than deleting outright.","title":"Steps"},{"location":"ceph-s3-intelligent-tiering-lifecycle-management/#creation-of-new-storage-class-and-pool","text":"Note: If you only wish to create a rule that expires and deletes objects, rather than moving them to a new storage tier, this section can be skipped, and you can move directly to \u201cCreating S3 lifecycle management policies\u201d This guide assumes the default RGW configuration. If you have a custom zonegroup or zones and placement targets set up you will have to replace the default names for your configured names. To find out if you are using the default configuration, run the following commands: root@labosd1:~# radosgw-admin zonegroup list root@labosd1:~# radosgw-admin zone list root@labosd1:~# radosgw-admin zonegroup placement list For a default configuration, it will look like this: If the output is different, be sure to replace \u201cdefault\u201d and \u201cdefault-placement\u201d with the correct naming scheme in the guide that follows.","title":"Creation of new storage class and pool"},{"location":"ceph-s3-intelligent-tiering-lifecycle-management/#add-a-new-storage-class","text":"First, create a new storage class within the Ceph RGW structure so a new pool can be created and then assign that pool to the new class. This guide is going to use the name \u201cCOLD\u201d for its storage class. You may want to instead use a storage class that AWS uses. They have predefined storage classes such as 1A-Standard. you Can find them here root@labosd1:~# radosgw-admin zonegroup placement add \\\\ --rgw-zonegroup default \\\\ --placement-id default-placement \\\\ --storage-class COLD Next, provide the zone placement info for the storage class that was just created. Also, set the name of the pool that the storage class \u201cCOLD\u201d is going to be tied to. The pool does not have to be created yet. It can be done in the next step. root@labosd1:~# radosgw-admin zone placement add \\\\ --rgw-zone default \\\\ --placement-id default-placement \\\\ --storage-class COLD \\\\ --data-pool default.rgw.cold.data \\\\ --compression lz4 The output should show that under storage classes \u201cCOLD\u201d the new data pool is assigned to it. Next, create the new pool. This can be done via the dashboard for ease where you can select the pool type (replicated or erasure coded) the number of placement groups, the type of compression, etc. Just make sure the pool name matches the one used in the command above. The new storage class is now complete and ready to use. The next steps are to create and design a lifecycle to do exactly what you want it to do. This guide will use a few simple examples.","title":"Add a new storage class"},{"location":"ceph-s3-intelligent-tiering-lifecycle-management/#creating-s3-lifecycle-management-policies","text":"The next step will require s3cmd to be installed on the machine. You can also use a tool like S3 browser for Windows which allows you to create s3 ACL policies. This guide will use s3cmd. Install s3cmd and then configure it using their built-in tool using \u2013configure. Enter the access key and secret key for the s3 bucket the s3 policy will be built on, and point it to the proper RGW endpoint and port. The below example will provide formatting for settings. Ensure for \u201cDNS-style bucket+hostname:port template\u201d is simply the IP address of your RGW and the port it uses without the bucket or location vars used. Default region is not needed. You can simply press enter on default region and encryption. At the end of the config, s3cmd will ask to test the settings. If it succeeds, you can continue. S3cmd will generate a config file at /root/.s3cfg. root@octrgw1:~# apt install s3cmd root@octrgw1:~# s3cmd \u2013configure To test functionality, run s3cmd ls. This should list the buckets tied to the provided access key and secret key.","title":"Creating S3 Lifecycle management policies"},{"location":"ceph-s3-intelligent-tiering-lifecycle-management/#lifecycle-configuration","text":"With the storage class created and s3cmd configured, we can now create some lifecycle management rules. These management rules are to be created in an xml file which can then be passed to the bucket with s3cmd. A more comprehensive list on lifecycle rules can be found here . However, please note this page is directed towards Amazon S3 and so remember some things may not carry over 1 to 1 with Ceph, although most of the rules should. With the lifecycle management tool, you have the option for \u201ctransition\u201d and \u201cexpiration\u201d You can create rules that have 2 steps that incorporates both. In order to create these lifecycle rules, create an XML file and create your rule within that XML file. For example, a rule can be made to move objects to a new storage class after 1 month, and then expire them to be deleted after 2. That lifecycle rule would look something like this: <LifecycleConfiguration> <Rule> <ID>Transition then Expire Rule</ID> <Filter> <Prefix></Prefix> </Filter> <Status>Enabled</Status> <Transition> <Days>30</Days> <StorageClass>COLD</StorageClass> </Transition> <Expiration> <Days>60</Days> </Expiration> </Rule> </LifecycleConfiguration> For a simpler rule, you can dictate simply to delete objects older than a set date. This rule would look similar to this: <LifecycleConfiguration> <Rule> <ID>Expire after 1 year</ID> <Filter> <Prefix></Prefix> </Filter> <Status>Enabled</Status> <Expiration> <Days>365</Days> </Expiration> </Rule> </LifecycleConfiguration> * Finally, for a rule to only move objects to a new storage class, and not mark them to be expired, you could use a rule like this: <LifecycleConfiguration> <Rule> <ID>Transition objects to COLD storage class</ID> <Filter> <Prefix></Prefix> </Filter> <Status>Enabled</Status> <Transition> <Days>30</Days> <StorageClass>COLD</StorageClass> </Transition> </Rule> </LifecycleConfiguration> * Inside these rules you will also find \u201cPrefix\u201d. This allows flexibility to have different rules running all on a single bucket and as long as the objects are uploaded with the correct prefix, the rule will only apply to it. This can allow you to have multiple expire rules within a single bucket. This would look something like this: <LifecycleConfiguration> <Rule> <ID>prefix expiration</ID> <Filter> <Prefix>dir1</Prefix> </Filter> <Status>Enabled</Status> <Expiration> <Days>120</Days> </Expiration> <Filter> <Prefix>dir2</Prefix> </Filter> <Status>Enabled</Status> <Expiration> <Days>60</Days> </Expiration> </Rule> </LifecycleConfiguration> * The above rule will expire all objects after 120 days with the prefix of dir1, and will expire all objects after 60 days with a prefix of dir2. * To put created rules in effect, create a new xml file on the same host s3cmd is installed. * Copy any rule above along with any changes needed for your environment. * Example: Save the file. This file is saved as lifecycle-expire.xml . The s3 bucket that this policy will be attached to is called lifecycletest. Using s3cmd, set the lifecycle policy on the correct bucket. root@octrgw1:~# s3cmd setlifecycle lifecycle-expire.xml s3://lifecycletest Test to see if the lifecycle has been set. root@octrgw1:~# s3cmd info s3://lifecycletest When using transition rules instead of expire rules, s3cmd info will not properly list it. You will need a tool like S3 browser to view the lifecycle rule. S3 browser can also be used to create these rules. The only downside of using S3 Browser is that it does not allow the use of custom storage classes such as the use of \u201cCOLD\u201d. It uses only standard Amazon S3 naming, so when using transition rather than expiration it is still better to use s3cmd and the XML files. A guide for s3 browser lifecycle management can be found here: https://s3browser.com/bucket-lifecycle-configuration.aspx","title":"Lifecycle configuration"},{"location":"ceph-s3-intelligent-tiering-lifecycle-management/#verification","text":"If you would like to create and test some of these rules, it is best to use Ceph\u2019s built in RGW debug mode. This allows a set period of time (typically 60 seconds) will correlate to 24 hours. This allows fast testing of rules to ensure they are doing exactly what is expected, and gives the ability to iterate quickly. To do this, you must SSH into your nodes acting as your RGW\u2019s. Once inside, open ceph.conf with sudo permissions. It can be found at /etc/ceph/ceph.conf Add the following line to the bottom of the conf file: rgw lc debug interval = 60 This sets the lifecycle interval to correlate every 60 seconds to 24 hours. Make sure if you are using multiple RGW\u2019s to add this to each RGW\u2019s config file. Once the config file is saved, restart the RGW\u2019s. Replace the name octrgw1 with the name of your RGW. root@octrgw1:~# systemctl restart ceph-radosgw@rgw.octrgw1 Troubleshooting =============== If you are having trouble with your lifecycle rules, it is most likely because the rules are written incorrectly, or the RGW debug interval is not working. In order to check the status of your RGW lifecycle rules, run the following command on a node with an admin keyring on it. root@octrgw1:~# radosgw-admin lc list This lists the lifecycle rules you have in place. If they are not set to \u201ccomplete\u201d you can force Ceph to start the rule by running the following command: root@octrgw1:~# radosgw-admin lc process","title":"Verification"},{"location":"cephbasedstoragetoakubernetescluster/","text":"Ceph-based storage to a Kubernetes cluster \u00b6 A practical example of connecting Ceph-based storage to a Kubernetes cluster \u00b6 So, you have a Kubernetes cluster at hand, deployed, for example, kubespray . A Ceph cluster works nearby - you can also install it, for example, with this set of playbooks . I hope I don't need to mention that for production, there must be a network between them with a bandwidth of at least 10 Gb / s. If you have all this, let's go! First, let's go to one of the nodes of the Ceph cluster and check that everything is in order: ceph health ceph -s Next, we will immediately create a pool for RBD disks: ceph osd pool create kube 32 ceph osd pool application enable kube rbd Let's go to the Kubernetes cluster. There, first of all, we will install the Ceph CSI driver for RBD. We will install, as expected, through Helm. Add a repository with a chart, get a set of ceph-csi-rbd chart variables: helm repo add ceph-csi https://ceph.github.io/csi-charts helm inspect values \u200b\u200bceph-csi/ceph-csi-rbd > cephrbd.yml Now you need to fill in the cephrbd.yml file. To do this, we find out the cluster ID and IP addresses of monitors in Ceph: ceph fsid # this is how we get the clusterID ceph mon dump # this is how we see the IP addresses of the monitors The resulting values \u200b\u200bare entered into the cephrbd.yml file. Along the way, we enable the creation of PSP policies (Pod Security Policies). The options in the nodeplugin and provisioner sections are already in the file, you can fix them as shown below: csiConfig: - clusterID: \"bcd0d202-fba8-4352-b25d-75c89258d5ab\" monitors: - \"v2:172.18.8.5:3300/0,v1:172.18.8.5:6789/0\" - \"v2:172.18.8.6:3300/ 0,v1:172.18.8.6:6789/0\" - \"v2:172.18.8.7:3300/0,v1:172.18.8.7:6789/0\" nodeplugin: subSecurityPolicy: enabled: true provisioner: podSecurityPolicy: enabled: true Then all that remains for us is to install the chart in the Kubernetes cluster. helm upgrade -i ceph-csi-rbd ceph-csi/ceph-csi-rbd -f cephrbd.yml -n ceph-csi-rbd --create-namespace Great, the RBD driver works! Let's create a new StorageClass in Kubernetes. This again requires some work with Ceph. Create a new user in Ceph and grant him write access to the kube pool: ceph auth get-or-create client.rbdkube mon 'profile rbd' osd 'profile rbd pool=kube' And now let's see the access key is still there: ceph auth get-key client.rbdkube The command will output something like this: AQCO9NJbhYipKRAAMqZsnqqS/T8OYQX20xIa9A== Let's put this value in Secret in the Kubernetes cluster - where userKey is needed: --- apiVersion: v1 kind: Secret metadata: name: csi-rbd-secret namespace: ceph-csi-rbd stringData: # The key values \u200b\u200bcorrespond to the username and its key, as specified in # the Ceph cluster. The user ID must have access to the pool # specified in storage class userID: rbdkube userKey: <user-key> And we create our secret: kubectl apply -f secret.yaml Next, we need something like this StorageClass manifest: --- apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: csi-rbd-sc provisioner: rbd.csi.ceph.com parameters: clusterID: <cluster-id> pool: kube image Features: layering # These secrets should contain data to authorize # into your pool. csi.storage.k8s.io/provisioner-secret-name: csi-rbd-secret csi.storage.k8s.io/provisioner-secret-namespace: ceph-csi-rbd csi.storage.k8s.io/controller-expand- secret-name: csi-rbd-secret csi.storage.k8s.io/controller-expand-secret-namespace: ceph-csi-rbd csi.storage.k8s.io/node-stage-secret-name: csi-rbd- secret csi.storage.k8s.io/node-stage-secret-namespace: ceph-csi-rbd csi.storage.k8s.io/fstype:ext4 reclaimPolicy: Delete allowVolumeExpansion: true mountOptions: - discard We need to fill in clusterID , which we already learned with the ceph fsid command, and apply this manifest in the Kubernetes cluster: kubectl apply -f storageclass.yaml To check the operation of clusters in a bundle, let's create the following PVC (Persistent Volume Claim): apiVersion: v1 kind: PersistentVolumeClaim metadata: name: rbd-pvc spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi storageClassName: csi-rbd-sc Let's immediately see how Kubernetes created the requested volume in Ceph: kubectl get pvc kubectl get pv Everything seems to be great! And what does it look like on the Ceph side? We get a list of volumes in the pool and view information about our volume: rbd ls -p kube rbd -p kube info csi-vol-eb3d257d-8c6c-11ea-bff5-6235e7640653 # here, of course, there will be a different volume ID that the previous command issued Now let's see how RBD volume resizing works. Change the volume size in the pvc.yaml manifest to 2Gi and apply it: kubectl apply -f pvc.yaml Let's wait for the changes to take effect and take another look at the volume size. rbd -p kube info csi-vol-eb3d257d-8c6c-11ea-bff5-6235e7640653 kubectl get pv kubectl get pvc We see that the size of the PVC has not changed. You can ask Kubernetes for a description of the PVC in YAML format to find out why: kubectl get pvc rbd-pvc -o yaml And here is the problem: message: Waiting for user to (re-)start a pod to finish file system resize of volume on node. type: FileSystemResizePending That is, the disk has grown, but the file system on it has not. To expand the file system, you need to mount the volume. In our case, the created PVC / PV is not used in any way now. We can create a test Pod like this: --- apiVersion: v1 kind: Pod metadata: name: csi-rbd-demo-pod spec: containers: - name: web-server image: nginx:1.17.6 volumeMounts: - name: mypvc mountPath: /data volumes: - name: mypvc persistentVolumeClaim: claimName: rbd-pvc readOnly: false And now let's look at PVC: kubectl get pvc The size has changed, everything is in order. In the first part, we worked with the RBD block device (it stands for Rados Block Device), but this cannot be done if you want to work with this disk of different microservices at the same time. For working with files, rather than with a disk image, CephFS is much better suited. Using the example of Ceph and Kubernetes clusters, we will configure CSI and other necessary entities to work with CephFS. Let's get the values \u200b\u200bfrom the new Helm chart we need: helm inspect values \u200b\u200bceph-csi/ceph-csi-cephfs > cephfs.yml Again, you need to fill in the cephfs.yml file. As before, the Ceph commands will help: ceph fsid ceph mon dump We fill the file with values \u200b\u200blike this: csiConfig: - clusterID: \"bcd0d202-fba8-4352-b25d-75c89258d5ab\" monitors: - \"172.18.8.5:6789\" - \"172.18.8.6:6789\" - \"172.18.8.7:6789\" nodeplugin: httpMetrics: enabled: true containerPort: 8091 podSecurityPolicy: enabled: true provisioner: replicaCount: 1 podSecurityPolicy: enabled: true Note that monitor addresses are specified in the simple form address:port. To mount cephfs on the host, these addresses are passed to a kernel module that does not yet know how to work with the monitor protocol v2. We change the port for httpMetrics (Prometheus will go there for monitoring metrics) so that it does not conflict with nginx-proxy, which is installed by Kubespray. You may not need this. Install Helm chart in Kubernetes cluster: helm upgrade -i ceph-csi-cephfs ceph-csi/ceph-csi-cephfs -f cephfs.yml -n ceph-csi-cephfs --create-namespace Let's move on to the Ceph data store to create a separate user there. The documentation states that the CephFS provider needs cluster administrator access rights. But we will create a separate fs user with limited rights: ceph auth get-or-create client.fs mon 'allow r' mgr 'allow rw' mds 'allow rws' osd 'allow rw pool=cephfs_data, allow rw pool=cephfs_metadata' And immediately let's see its access key, it will be useful to us further: ceph auth get-key client.fs Let's create separate Secret and StorageClass. Nothing new, we have already seen this with the example of RBD: --- apiVersion: v1 kind: Secret metadata: name: csi-cephfs-secret namespace: ceph-csi-cephfs stringData: # Required for dynamically created volumes adminID: fs adminKey: <previous command output> Applying the manifest: kubectl apply -f secret.yaml And now - a separate StorageClass: --- apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: csi-cephfs-sc provisioner: cephfs.csi.ceph.com parameters: clusterID: <cluster-id> # Name of the CephFS file system where the volume will be created fsName: cephfs # (optional) The Ceph pool where the volume data will be stored # pool: cephfs_data # (optional) Ceph-fuse comma-separated mount options # ex: # fuseMountOptions: debug # (optional) Comma-separated CephFS kernel mount options # See man mount.ceph for a list of these options. For example: # kernelMountOptions: readdir_max_bytes=1048576,norbytes # Secrets must contain permissions for the Ceph admin and/or user. csi.storage.k8s.io/provisioner-secret-name: csi-cephfs-secret csi.storage.k8s.io/provisioner-secret-namespace: ceph-csi-cephfs csi.storage.k8s.io/controller-expand- secret-name: csi-cephfs-secret csi.storage.k8s.io/controller-expand-secret-namespace: ceph-csi-cephfs csi.storage.k8s.io/node-stage-secret-name: csi-cephfs- secret csi.storage.k8s.io/node-stage-secret-namespace: ceph-csi-cephfs # (optional) The driver can use either ceph-fuse (fuse), # or ceph kernelclient (kernel). # If not specified, default volume mounts will be used, # determined by searching ceph-fuse and mount.ceph # mounter: kernel reclaimPolicy: Delete allowVolumeExpansion: true mountOptions: - debug Fill in clusterID here and apply it in Kubernetes: kubectl apply -f storageclass.yaml Check \u00b6 To check, as in the previous example, let's create a PVC: --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: csi-cephfs-pvc spec: accessModes: - ReadWriteMany resources: requests: storage: 5Gi storageClassName: csi-cephfs-sc And check for PVC/PV: kubectl get pvc kubectl get pv If you want to look at the files and directories in CephFS, you can mount this file system somewhere. For example, as shown below. We go to one of the nodes of the Ceph cluster and perform the following actions: # Mount point mkdir -p /mnt/cephfs # Create a file with the ceph auth admin key get-key client.admin >/etc/ceph/secret.key # Add an entry to /etc/fstab # !! Change the ip address to our node address echo \"172.18.8.6:6789:/ /mnt/cephfs ceph name=admin,secretfile=/etc/ceph/secret.key,noatime,_netdev 0 2\" >> /etc/fstab mount /mnt/cephfs Of course, this FS mount on a Ceph node is only suitable for educational purposes,. I don't think anyone would do this in production, there's a big risk of accidentally overwriting important files. And finally, let's check how things work with volume resizing in the case of CephFS. We return to Kubernetes and edit our manifest for PVC - we increase the size there, for example, to 7Gi. Apply the edited file: kubectl apply -f pvc.yaml Let's see how the quota has changed on the mounted directory: getfattr -n ceph.quota.max_bytes <data-directory> You may need to install the attr package on your system for this command to work. The eyes are afraid, but the hands are doing \u00b6 On the surface, all these spells and long YAML manifests seem complicated, but in practice, Slurm students deal with them pretty quickly. In this article, we did not delve into the wilds - there is official documentation for this. If you are interested in the details of setting up Ceph storage with a Kubernetes cluster, these links will help: General principles of Kubernetes with volumes RDD documentation Integrating RBD and Kubernetes from a Ceph Perspective Integrating RBD and Kubernetes from a CSI Perspective General CephFS documentation CephFS and Kubernetes integration from a CSI","title":"Ceph-based storage to a Kubernetes cluster"},{"location":"cephbasedstoragetoakubernetescluster/#ceph-based-storage-to-a-kubernetes-cluster","text":"","title":"Ceph-based storage to a Kubernetes cluster"},{"location":"cephbasedstoragetoakubernetescluster/#a-practical-example-of-connecting-ceph-based-storage-to-a-kubernetes-cluster","text":"So, you have a Kubernetes cluster at hand, deployed, for example, kubespray . A Ceph cluster works nearby - you can also install it, for example, with this set of playbooks . I hope I don't need to mention that for production, there must be a network between them with a bandwidth of at least 10 Gb / s. If you have all this, let's go! First, let's go to one of the nodes of the Ceph cluster and check that everything is in order: ceph health ceph -s Next, we will immediately create a pool for RBD disks: ceph osd pool create kube 32 ceph osd pool application enable kube rbd Let's go to the Kubernetes cluster. There, first of all, we will install the Ceph CSI driver for RBD. We will install, as expected, through Helm. Add a repository with a chart, get a set of ceph-csi-rbd chart variables: helm repo add ceph-csi https://ceph.github.io/csi-charts helm inspect values \u200b\u200bceph-csi/ceph-csi-rbd > cephrbd.yml Now you need to fill in the cephrbd.yml file. To do this, we find out the cluster ID and IP addresses of monitors in Ceph: ceph fsid # this is how we get the clusterID ceph mon dump # this is how we see the IP addresses of the monitors The resulting values \u200b\u200bare entered into the cephrbd.yml file. Along the way, we enable the creation of PSP policies (Pod Security Policies). The options in the nodeplugin and provisioner sections are already in the file, you can fix them as shown below: csiConfig: - clusterID: \"bcd0d202-fba8-4352-b25d-75c89258d5ab\" monitors: - \"v2:172.18.8.5:3300/0,v1:172.18.8.5:6789/0\" - \"v2:172.18.8.6:3300/ 0,v1:172.18.8.6:6789/0\" - \"v2:172.18.8.7:3300/0,v1:172.18.8.7:6789/0\" nodeplugin: subSecurityPolicy: enabled: true provisioner: podSecurityPolicy: enabled: true Then all that remains for us is to install the chart in the Kubernetes cluster. helm upgrade -i ceph-csi-rbd ceph-csi/ceph-csi-rbd -f cephrbd.yml -n ceph-csi-rbd --create-namespace Great, the RBD driver works! Let's create a new StorageClass in Kubernetes. This again requires some work with Ceph. Create a new user in Ceph and grant him write access to the kube pool: ceph auth get-or-create client.rbdkube mon 'profile rbd' osd 'profile rbd pool=kube' And now let's see the access key is still there: ceph auth get-key client.rbdkube The command will output something like this: AQCO9NJbhYipKRAAMqZsnqqS/T8OYQX20xIa9A== Let's put this value in Secret in the Kubernetes cluster - where userKey is needed: --- apiVersion: v1 kind: Secret metadata: name: csi-rbd-secret namespace: ceph-csi-rbd stringData: # The key values \u200b\u200bcorrespond to the username and its key, as specified in # the Ceph cluster. The user ID must have access to the pool # specified in storage class userID: rbdkube userKey: <user-key> And we create our secret: kubectl apply -f secret.yaml Next, we need something like this StorageClass manifest: --- apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: csi-rbd-sc provisioner: rbd.csi.ceph.com parameters: clusterID: <cluster-id> pool: kube image Features: layering # These secrets should contain data to authorize # into your pool. csi.storage.k8s.io/provisioner-secret-name: csi-rbd-secret csi.storage.k8s.io/provisioner-secret-namespace: ceph-csi-rbd csi.storage.k8s.io/controller-expand- secret-name: csi-rbd-secret csi.storage.k8s.io/controller-expand-secret-namespace: ceph-csi-rbd csi.storage.k8s.io/node-stage-secret-name: csi-rbd- secret csi.storage.k8s.io/node-stage-secret-namespace: ceph-csi-rbd csi.storage.k8s.io/fstype:ext4 reclaimPolicy: Delete allowVolumeExpansion: true mountOptions: - discard We need to fill in clusterID , which we already learned with the ceph fsid command, and apply this manifest in the Kubernetes cluster: kubectl apply -f storageclass.yaml To check the operation of clusters in a bundle, let's create the following PVC (Persistent Volume Claim): apiVersion: v1 kind: PersistentVolumeClaim metadata: name: rbd-pvc spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi storageClassName: csi-rbd-sc Let's immediately see how Kubernetes created the requested volume in Ceph: kubectl get pvc kubectl get pv Everything seems to be great! And what does it look like on the Ceph side? We get a list of volumes in the pool and view information about our volume: rbd ls -p kube rbd -p kube info csi-vol-eb3d257d-8c6c-11ea-bff5-6235e7640653 # here, of course, there will be a different volume ID that the previous command issued Now let's see how RBD volume resizing works. Change the volume size in the pvc.yaml manifest to 2Gi and apply it: kubectl apply -f pvc.yaml Let's wait for the changes to take effect and take another look at the volume size. rbd -p kube info csi-vol-eb3d257d-8c6c-11ea-bff5-6235e7640653 kubectl get pv kubectl get pvc We see that the size of the PVC has not changed. You can ask Kubernetes for a description of the PVC in YAML format to find out why: kubectl get pvc rbd-pvc -o yaml And here is the problem: message: Waiting for user to (re-)start a pod to finish file system resize of volume on node. type: FileSystemResizePending That is, the disk has grown, but the file system on it has not. To expand the file system, you need to mount the volume. In our case, the created PVC / PV is not used in any way now. We can create a test Pod like this: --- apiVersion: v1 kind: Pod metadata: name: csi-rbd-demo-pod spec: containers: - name: web-server image: nginx:1.17.6 volumeMounts: - name: mypvc mountPath: /data volumes: - name: mypvc persistentVolumeClaim: claimName: rbd-pvc readOnly: false And now let's look at PVC: kubectl get pvc The size has changed, everything is in order. In the first part, we worked with the RBD block device (it stands for Rados Block Device), but this cannot be done if you want to work with this disk of different microservices at the same time. For working with files, rather than with a disk image, CephFS is much better suited. Using the example of Ceph and Kubernetes clusters, we will configure CSI and other necessary entities to work with CephFS. Let's get the values \u200b\u200bfrom the new Helm chart we need: helm inspect values \u200b\u200bceph-csi/ceph-csi-cephfs > cephfs.yml Again, you need to fill in the cephfs.yml file. As before, the Ceph commands will help: ceph fsid ceph mon dump We fill the file with values \u200b\u200blike this: csiConfig: - clusterID: \"bcd0d202-fba8-4352-b25d-75c89258d5ab\" monitors: - \"172.18.8.5:6789\" - \"172.18.8.6:6789\" - \"172.18.8.7:6789\" nodeplugin: httpMetrics: enabled: true containerPort: 8091 podSecurityPolicy: enabled: true provisioner: replicaCount: 1 podSecurityPolicy: enabled: true Note that monitor addresses are specified in the simple form address:port. To mount cephfs on the host, these addresses are passed to a kernel module that does not yet know how to work with the monitor protocol v2. We change the port for httpMetrics (Prometheus will go there for monitoring metrics) so that it does not conflict with nginx-proxy, which is installed by Kubespray. You may not need this. Install Helm chart in Kubernetes cluster: helm upgrade -i ceph-csi-cephfs ceph-csi/ceph-csi-cephfs -f cephfs.yml -n ceph-csi-cephfs --create-namespace Let's move on to the Ceph data store to create a separate user there. The documentation states that the CephFS provider needs cluster administrator access rights. But we will create a separate fs user with limited rights: ceph auth get-or-create client.fs mon 'allow r' mgr 'allow rw' mds 'allow rws' osd 'allow rw pool=cephfs_data, allow rw pool=cephfs_metadata' And immediately let's see its access key, it will be useful to us further: ceph auth get-key client.fs Let's create separate Secret and StorageClass. Nothing new, we have already seen this with the example of RBD: --- apiVersion: v1 kind: Secret metadata: name: csi-cephfs-secret namespace: ceph-csi-cephfs stringData: # Required for dynamically created volumes adminID: fs adminKey: <previous command output> Applying the manifest: kubectl apply -f secret.yaml And now - a separate StorageClass: --- apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: csi-cephfs-sc provisioner: cephfs.csi.ceph.com parameters: clusterID: <cluster-id> # Name of the CephFS file system where the volume will be created fsName: cephfs # (optional) The Ceph pool where the volume data will be stored # pool: cephfs_data # (optional) Ceph-fuse comma-separated mount options # ex: # fuseMountOptions: debug # (optional) Comma-separated CephFS kernel mount options # See man mount.ceph for a list of these options. For example: # kernelMountOptions: readdir_max_bytes=1048576,norbytes # Secrets must contain permissions for the Ceph admin and/or user. csi.storage.k8s.io/provisioner-secret-name: csi-cephfs-secret csi.storage.k8s.io/provisioner-secret-namespace: ceph-csi-cephfs csi.storage.k8s.io/controller-expand- secret-name: csi-cephfs-secret csi.storage.k8s.io/controller-expand-secret-namespace: ceph-csi-cephfs csi.storage.k8s.io/node-stage-secret-name: csi-cephfs- secret csi.storage.k8s.io/node-stage-secret-namespace: ceph-csi-cephfs # (optional) The driver can use either ceph-fuse (fuse), # or ceph kernelclient (kernel). # If not specified, default volume mounts will be used, # determined by searching ceph-fuse and mount.ceph # mounter: kernel reclaimPolicy: Delete allowVolumeExpansion: true mountOptions: - debug Fill in clusterID here and apply it in Kubernetes: kubectl apply -f storageclass.yaml","title":"A practical example of connecting Ceph-based storage to a Kubernetes cluster"},{"location":"cephbasedstoragetoakubernetescluster/#check","text":"To check, as in the previous example, let's create a PVC: --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: csi-cephfs-pvc spec: accessModes: - ReadWriteMany resources: requests: storage: 5Gi storageClassName: csi-cephfs-sc And check for PVC/PV: kubectl get pvc kubectl get pv If you want to look at the files and directories in CephFS, you can mount this file system somewhere. For example, as shown below. We go to one of the nodes of the Ceph cluster and perform the following actions: # Mount point mkdir -p /mnt/cephfs # Create a file with the ceph auth admin key get-key client.admin >/etc/ceph/secret.key # Add an entry to /etc/fstab # !! Change the ip address to our node address echo \"172.18.8.6:6789:/ /mnt/cephfs ceph name=admin,secretfile=/etc/ceph/secret.key,noatime,_netdev 0 2\" >> /etc/fstab mount /mnt/cephfs Of course, this FS mount on a Ceph node is only suitable for educational purposes,. I don't think anyone would do this in production, there's a big risk of accidentally overwriting important files. And finally, let's check how things work with volume resizing in the case of CephFS. We return to Kubernetes and edit our manifest for PVC - we increase the size there, for example, to 7Gi. Apply the edited file: kubectl apply -f pvc.yaml Let's see how the quota has changed on the mounted directory: getfattr -n ceph.quota.max_bytes <data-directory> You may need to install the attr package on your system for this command to work.","title":"Check"},{"location":"cephbasedstoragetoakubernetescluster/#the-eyes-are-afraid-but-the-hands-are-doing","text":"On the surface, all these spells and long YAML manifests seem complicated, but in practice, Slurm students deal with them pretty quickly. In this article, we did not delve into the wilds - there is official documentation for this. If you are interested in the details of setting up Ceph storage with a Kubernetes cluster, these links will help: General principles of Kubernetes with volumes RDD documentation Integrating RBD and Kubernetes from a Ceph Perspective Integrating RBD and Kubernetes from a CSI Perspective General CephFS documentation CephFS and Kubernetes integration from a CSI","title":"The eyes are afraid, but the hands are doing"},{"location":"howtobuildpodmanversion/","text":"How to install and Build Podman in Debian/ubuntu \u00b6 ====================================== podman is a tool for managing (Kubernetes) pods, containers and container images, available from https://github.com/containers/libpod To install podman from scratch in Debian/ubuntu we need to have the below requirement met golang >= 1.8 (stretch+) docker-runc conmon CNI networking ostree (Packaged) libapparmor-dev Let's begin with the Installation First, update the system apt -y update Install Required dependency to build various podman related go lang library apt -y install \\ vim \\ curl \\ gcc \\ make \\ cmake \\ git \\ btrfs-progs \\ golang-go \\ go-md2man \\ iptables \\ libassuan-dev \\ libc6-dev \\ libdevmapper-dev \\ libglib2.0-dev \\ libgpgme-dev \\ libgpg-error-dev \\ libostree-dev \\ libprotobuf-dev \\ libprotobuf-c-dev \\ libseccomp-dev \\ libselinux1-dev \\ libsystemd-dev \\ pkg-config \\ runc \\ uidmap \\ libapparmor-dev check the runc version The runc should at least spec: 1.0.1 otherwise you need to build your one [email protected]:~# runc -version runc version spec: 1.0.1-dev install GOLANG export GOPATH=~/go git clone https://go.googlesource.com/go $GOPATH cd $GOPATH git checkout tags/go1.10.8 # optional cd src ./all.bash export PATH=$GOPATH/bin:$PATH Install conmon conmon is used to monitor OCI Runtimes. To build from source, use the following: cd ~ git clone https://github.com/containers/conmon cd conmon export GOCACHE=\"$(mktemp -d)\" make sudo make podman sudo cp /usr/local/libexec/podman/conmon /usr/local/bin/ Configure CNI A basic network configuration can be achieved with: sudo mkdir -p /etc/cni/net.d curl -qsSL https://raw.githubusercontent.com/containers/libpod/master/cni/87-podman-bridge.conflist | sudo tee /etc/cni/net.d/87-podman-bridge.conf The directory containing CNI plugin configuration files cni_config_dir = \"/etc/cni/net.d/\" Install CNI Plugins cd ~ git clone https://github.com/containernetworking/plugins.git $GOPATH/src/github.com/containernetworking/plugins cd $GOPATH/src/github.com/containernetworking/plugins ./build_linux.sh sudo mkdir -p /usr/libexec/cni sudo cp bin/* /usr/libexec/cni Others Directories where the CNI plugin binaries may be located cni_plugin_dir = [ \"/usr/libexec/cni\", \"/usr/lib/cni\", \"/usr/local/lib/cni\", \"/opt/cni/bin\" ] Add Podman configuration registries,policy cd ~ sudo mkdir -p /etc/containers sudo curl https://raw.githubusercontent.com/projectatomic/registries/master/registries.fedora -o /etc/containers/registries.conf sudo curl https://raw.githubusercontent.com/containers/skopeo/master/default-policy.json -o /etc/containers/policy.json Install PODMAN cd ~ git clone https://github.com/containers/libpod/ $GOPATH/src/github.com/containers/libpod cd $GOPATH/src/github.com/containers/libpod make BUILDTAGS=\"selinux seccomp\" sudo make install PREFIX=/usr Test Podman [email protected]:~# podman help manage pods and images Usage: podman [flags] podman [command] Available Commands: attach Attach to a running container build Build an image using instructions from Containerfiles commit Create new image based on the changed container container Manage Containers cp Copy files/folders between a container and the local filesystem create Create but do not start a container diff Inspect changes on container's file systems events Show podman events exec Run a process in a running container export Export container's filesystem contents as a tar archive generate Generated structured data healthcheck Manage Healthcheck help Help about any command history Show history of a specified image image Manage images images List images in local storage import Import a tarball to create a filesystem image info Display podman system information init Initialize one or more containers inspect Display the configuration of a container or image kill Kill one or more running containers with a specific signal load Load an image from container archive login Login to a container registry logout Logout of a container registry logs Fetch the logs of a container mount Mount a working container's root filesystem network Manage Networks pause Pause all the processes in one or more containers play Play a pod pod Manage pods port List port mappings or a specific mapping for the container ps List containers pull Pull an image from a registry push Push an image to a specified destination restart Restart one or more containers rm Remove one or more containers rmi Removes one or more images from local storage run Run a command in a new container save Save image to an archive search Search registry for image start Start one or more containers stats Display a live stream of container resource usage statistics stop Stop one or more containers system Manage podman tag Add an additional name to a local image top Display the running processes of a container umount Unmounts working container's root filesystem unpause Unpause the processes in one or more containers unshare Run a command in a modified user namespace varlink Run varlink interface version Display the Podman Version Information volume Manage volumes wait Block on one or more containers","title":"How to install and Build Podman in Debian/ubuntu"},{"location":"howtobuildpodmanversion/#how-to-install-and-build-podman-in-debianubuntu","text":"====================================== podman is a tool for managing (Kubernetes) pods, containers and container images, available from https://github.com/containers/libpod To install podman from scratch in Debian/ubuntu we need to have the below requirement met golang >= 1.8 (stretch+) docker-runc conmon CNI networking ostree (Packaged) libapparmor-dev Let's begin with the Installation First, update the system apt -y update Install Required dependency to build various podman related go lang library apt -y install \\ vim \\ curl \\ gcc \\ make \\ cmake \\ git \\ btrfs-progs \\ golang-go \\ go-md2man \\ iptables \\ libassuan-dev \\ libc6-dev \\ libdevmapper-dev \\ libglib2.0-dev \\ libgpgme-dev \\ libgpg-error-dev \\ libostree-dev \\ libprotobuf-dev \\ libprotobuf-c-dev \\ libseccomp-dev \\ libselinux1-dev \\ libsystemd-dev \\ pkg-config \\ runc \\ uidmap \\ libapparmor-dev check the runc version The runc should at least spec: 1.0.1 otherwise you need to build your one [email protected]:~# runc -version runc version spec: 1.0.1-dev install GOLANG export GOPATH=~/go git clone https://go.googlesource.com/go $GOPATH cd $GOPATH git checkout tags/go1.10.8 # optional cd src ./all.bash export PATH=$GOPATH/bin:$PATH Install conmon conmon is used to monitor OCI Runtimes. To build from source, use the following: cd ~ git clone https://github.com/containers/conmon cd conmon export GOCACHE=\"$(mktemp -d)\" make sudo make podman sudo cp /usr/local/libexec/podman/conmon /usr/local/bin/ Configure CNI A basic network configuration can be achieved with: sudo mkdir -p /etc/cni/net.d curl -qsSL https://raw.githubusercontent.com/containers/libpod/master/cni/87-podman-bridge.conflist | sudo tee /etc/cni/net.d/87-podman-bridge.conf The directory containing CNI plugin configuration files cni_config_dir = \"/etc/cni/net.d/\" Install CNI Plugins cd ~ git clone https://github.com/containernetworking/plugins.git $GOPATH/src/github.com/containernetworking/plugins cd $GOPATH/src/github.com/containernetworking/plugins ./build_linux.sh sudo mkdir -p /usr/libexec/cni sudo cp bin/* /usr/libexec/cni Others Directories where the CNI plugin binaries may be located cni_plugin_dir = [ \"/usr/libexec/cni\", \"/usr/lib/cni\", \"/usr/local/lib/cni\", \"/opt/cni/bin\" ] Add Podman configuration registries,policy cd ~ sudo mkdir -p /etc/containers sudo curl https://raw.githubusercontent.com/projectatomic/registries/master/registries.fedora -o /etc/containers/registries.conf sudo curl https://raw.githubusercontent.com/containers/skopeo/master/default-policy.json -o /etc/containers/policy.json Install PODMAN cd ~ git clone https://github.com/containers/libpod/ $GOPATH/src/github.com/containers/libpod cd $GOPATH/src/github.com/containers/libpod make BUILDTAGS=\"selinux seccomp\" sudo make install PREFIX=/usr Test Podman [email protected]:~# podman help manage pods and images Usage: podman [flags] podman [command] Available Commands: attach Attach to a running container build Build an image using instructions from Containerfiles commit Create new image based on the changed container container Manage Containers cp Copy files/folders between a container and the local filesystem create Create but do not start a container diff Inspect changes on container's file systems events Show podman events exec Run a process in a running container export Export container's filesystem contents as a tar archive generate Generated structured data healthcheck Manage Healthcheck help Help about any command history Show history of a specified image image Manage images images List images in local storage import Import a tarball to create a filesystem image info Display podman system information init Initialize one or more containers inspect Display the configuration of a container or image kill Kill one or more running containers with a specific signal load Load an image from container archive login Login to a container registry logout Logout of a container registry logs Fetch the logs of a container mount Mount a working container's root filesystem network Manage Networks pause Pause all the processes in one or more containers play Play a pod pod Manage pods port List port mappings or a specific mapping for the container ps List containers pull Pull an image from a registry push Push an image to a specified destination restart Restart one or more containers rm Remove one or more containers rmi Removes one or more images from local storage run Run a command in a new container save Save image to an archive search Search registry for image start Start one or more containers stats Display a live stream of container resource usage statistics stop Stop one or more containers system Manage podman tag Add an additional name to a local image top Display the running processes of a container umount Unmounts working container's root filesystem unpause Unpause the processes in one or more containers unshare Run a command in a modified user namespace varlink Run varlink interface version Display the Podman Version Information volume Manage volumes wait Block on one or more containers","title":"How to install and Build Podman in Debian/ubuntu"},{"location":"installingbuildahandpodman/","text":"Installing buildah and podman on Ubuntu \u00b6 Buildah: Installing buildah and podman on Ubuntu 20.04 buildah is a tool that can create OCI compatible (Docker) images without the Docker daemon. podman can run these images without the need for a Docker daemon. The buildah and podman packages are available on the Debian Bullseye testing branch, but are not available from an Ubuntu 20.04 LTS release because these test packages could break the library compatibilities of the overall system. So instead, we will add the targeted remote repository as shown below. prereq packages \u00b6 sudo apt-get update sudo apt-get install -y wget ca-certificates gnupg2 add repo and signing key \u00b6 VERSION\\_ID=$(lsb\\_release -r | cut -f2) echo \"deb http://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/xUbuntu\\_${VERSION\\_ID}/ /\" | sudo tee /etc/apt/sources.list.d/devel-kubic-libcontainers-stable.list curl -Ls https://download.opensuse.org/repositories/devel:kubic:libcontainers:stable/xUbuntu\\_$VERSION\\_ID/Release.key | sudo apt-key add - sudo apt-get update install buildah and podman \u00b6 sudo apt install buildah podman -y fix known issue 11745 with [machine] entry \u00b6 sudo sed -i 's/^\\\\\\[machine\\\\\\]$/#\\\\\\[machine\\\\\\]/' /usr/share/containers/containers.conf Smoke test the installation with the following commands. sanity test \u00b6 buildah version podman version Create image with buildah \u00b6 To test buildah, we will create a small NGINX container that delivers custom content. Create custom index.html echo 'Hello world!' > /tmp/index.html Pull base NGINX image buildah pull docker.io/nginx:1.23.1-alpine $ buildah images REPOSITORY TAG IMAGE ID CREATED SIZE docker.io/library/nginx 1.23.1-alpine e46bcc697531 13 days ago 25 MB Build and commit image define base image \u00b6 nginx_id=$(buildah from docker.io/nginx:1.23.1-alpine) copy custom content \u00b6 buildah copy $nginx\\_id /tmp/index.html /usr/share/nginx/html outward facing default port \u00b6 buildah config --port 8080 $nginx\\_id commit into local repository \u00b6 buildah commit $nginx\\_id nginx-alpine-custom $ buildah images | REPOSITORY | TAG | IMAGE ID | CREATED | SIZE | | ------------------------------- | --------------- | -------------- | --------------- | ----- | | localhost/nginx-alpine-custom | latest | 94d26c327087 | 3 seconds ago | 25 MB | | ------------------------------- | --------------- | -------------- | --------------- | ----- | | docker.io/library/nginx | 1.23.1-alpine | e46bcc697531 | 13 days ago | 25 MB | | ------------------------------- | --------------- | -------------- | --------------- | ----- | Run local image with podman \u00b6 With the image now in the local repository, we can reference and run it with podman directly. podman run -p 8080:80 --name nginxtest localhost/nginx-alpine-custom:latest This will start a container in the foreground, exposing port 8080 to our host. Validate running image \u00b6 From another console, run a simple curl to port 8080 to see the custom index.html content we placed into the image. $ curl http://localhost:8080/index.html Hello world! Cleanup \u00b6 this command OR Ctrl-C from console where it is running in foreground podman stop nginxtest remove exited container podman rm nginxtest Alternative: build from Dockerfile \u00b6 Running a series of buildah commands is not the only way to generate the image. You can also use the Dockerfile format, here is the equivalent of the buildah commands run earlier. $ cat Dockerfile FROM docker.io/nginx:1.23.1-alpine COPY index.html /usr/share/nginx/html EXPOSE 8080 And the custom index.html file that needs to be located in the same directory. $ cat index.html Hello world from Dockerfile! Then you can \u2018buildah bud\u2019 (build-using-dockerfile) using the following command. example of tagging with both 'latest' and '1.0' tags \u00b6 buildah bud -f Dockerfile -t nginx-alpine-custom2:latest -t nginx-alpine-custom2:1.0 $ buildah images REPOSITORY TAG IMAGE ID CREATED SIZE localhost/nginx-alpine-custom2 latest c09a19d7737b 9 minutes ago 25 MB localhost/nginx-alpine-custom2 1.0 c09a19d7737b 9 minutes ago 25 MB Push image to Docker Hub \u00b6 If you wanted to push this image to a remote repository such as Docker Hub at docker.io, then you need to tag the image first. buildah tag localhost/springbootwithbuildah:latest docker.io/fabianlee/springbootwithbuildah:1.0 Then login with your Docker hub credentials and do a \u2018buildah push\u2019. $ echo 'YourPassw0rd' | buildah login --username fabianlee --password-stdin docker.io Login Succeeded! $ buildah push docker.io/fabianlee/springbootwithbuildah:1.0 Getting image source signatures Copying blob 4a5edb57ddd4 skipped: already exists Copying blob 735956b91a18 skipped: already exists Copying blob f3b732ca7f91 skipped: already exists Copying blob 5ce792dcacff [--------------------] 0.0b/0.0b Copying config 65a80bbe96 [--------------------] 0.0b/5.4KiB Writing manifest to image destination","title":"Installing buildah and podman on Ubuntu"},{"location":"installingbuildahandpodman/#installing-buildah-and-podman-on-ubuntu","text":"Buildah: Installing buildah and podman on Ubuntu 20.04 buildah is a tool that can create OCI compatible (Docker) images without the Docker daemon. podman can run these images without the need for a Docker daemon. The buildah and podman packages are available on the Debian Bullseye testing branch, but are not available from an Ubuntu 20.04 LTS release because these test packages could break the library compatibilities of the overall system. So instead, we will add the targeted remote repository as shown below.","title":"Installing buildah and podman on Ubuntu"},{"location":"installingbuildahandpodman/#prereq-packages","text":"sudo apt-get update sudo apt-get install -y wget ca-certificates gnupg2","title":"prereq packages"},{"location":"installingbuildahandpodman/#add-repo-and-signing-key","text":"VERSION\\_ID=$(lsb\\_release -r | cut -f2) echo \"deb http://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/xUbuntu\\_${VERSION\\_ID}/ /\" | sudo tee /etc/apt/sources.list.d/devel-kubic-libcontainers-stable.list curl -Ls https://download.opensuse.org/repositories/devel:kubic:libcontainers:stable/xUbuntu\\_$VERSION\\_ID/Release.key | sudo apt-key add - sudo apt-get update","title":"add repo and signing key"},{"location":"installingbuildahandpodman/#install-buildah-and-podman","text":"sudo apt install buildah podman -y","title":"install buildah and podman"},{"location":"installingbuildahandpodman/#fix-known-issue-11745-with-machine-entry","text":"sudo sed -i 's/^\\\\\\[machine\\\\\\]$/#\\\\\\[machine\\\\\\]/' /usr/share/containers/containers.conf Smoke test the installation with the following commands.","title":"fix known issue 11745 with [machine] entry"},{"location":"installingbuildahandpodman/#sanity-test","text":"buildah version podman version","title":"sanity test"},{"location":"installingbuildahandpodman/#create-image-with-buildah","text":"To test buildah, we will create a small NGINX container that delivers custom content. Create custom index.html echo 'Hello world!' > /tmp/index.html Pull base NGINX image buildah pull docker.io/nginx:1.23.1-alpine $ buildah images REPOSITORY TAG IMAGE ID CREATED SIZE docker.io/library/nginx 1.23.1-alpine e46bcc697531 13 days ago 25 MB Build and commit image","title":"Create image with buildah"},{"location":"installingbuildahandpodman/#define-base-image","text":"nginx_id=$(buildah from docker.io/nginx:1.23.1-alpine)","title":"define base image"},{"location":"installingbuildahandpodman/#copy-custom-content","text":"buildah copy $nginx\\_id /tmp/index.html /usr/share/nginx/html","title":"copy custom content"},{"location":"installingbuildahandpodman/#outward-facing-default-port","text":"buildah config --port 8080 $nginx\\_id","title":"outward facing default port"},{"location":"installingbuildahandpodman/#commit-into-local-repository","text":"buildah commit $nginx\\_id nginx-alpine-custom $ buildah images | REPOSITORY | TAG | IMAGE ID | CREATED | SIZE | | ------------------------------- | --------------- | -------------- | --------------- | ----- | | localhost/nginx-alpine-custom | latest | 94d26c327087 | 3 seconds ago | 25 MB | | ------------------------------- | --------------- | -------------- | --------------- | ----- | | docker.io/library/nginx | 1.23.1-alpine | e46bcc697531 | 13 days ago | 25 MB | | ------------------------------- | --------------- | -------------- | --------------- | ----- |","title":"commit into local repository"},{"location":"installingbuildahandpodman/#run-local-image-with-podman","text":"With the image now in the local repository, we can reference and run it with podman directly. podman run -p 8080:80 --name nginxtest localhost/nginx-alpine-custom:latest This will start a container in the foreground, exposing port 8080 to our host.","title":"Run local image with podman"},{"location":"installingbuildahandpodman/#validate-running-image","text":"From another console, run a simple curl to port 8080 to see the custom index.html content we placed into the image. $ curl http://localhost:8080/index.html Hello world!","title":"Validate running image"},{"location":"installingbuildahandpodman/#cleanup","text":"this command OR Ctrl-C from console where it is running in foreground podman stop nginxtest remove exited container podman rm nginxtest","title":"Cleanup"},{"location":"installingbuildahandpodman/#alternative-build-from-dockerfile","text":"Running a series of buildah commands is not the only way to generate the image. You can also use the Dockerfile format, here is the equivalent of the buildah commands run earlier. $ cat Dockerfile FROM docker.io/nginx:1.23.1-alpine COPY index.html /usr/share/nginx/html EXPOSE 8080 And the custom index.html file that needs to be located in the same directory. $ cat index.html Hello world from Dockerfile! Then you can \u2018buildah bud\u2019 (build-using-dockerfile) using the following command.","title":"Alternative: build from Dockerfile"},{"location":"installingbuildahandpodman/#example-of-tagging-with-both-latest-and-10-tags","text":"buildah bud -f Dockerfile -t nginx-alpine-custom2:latest -t nginx-alpine-custom2:1.0 $ buildah images REPOSITORY TAG IMAGE ID CREATED SIZE localhost/nginx-alpine-custom2 latest c09a19d7737b 9 minutes ago 25 MB localhost/nginx-alpine-custom2 1.0 c09a19d7737b 9 minutes ago 25 MB","title":"example of tagging with both 'latest' and '1.0' tags"},{"location":"installingbuildahandpodman/#push-image-to-docker-hub","text":"If you wanted to push this image to a remote repository such as Docker Hub at docker.io, then you need to tag the image first. buildah tag localhost/springbootwithbuildah:latest docker.io/fabianlee/springbootwithbuildah:1.0 Then login with your Docker hub credentials and do a \u2018buildah push\u2019. $ echo 'YourPassw0rd' | buildah login --username fabianlee --password-stdin docker.io Login Succeeded! $ buildah push docker.io/fabianlee/springbootwithbuildah:1.0 Getting image source signatures Copying blob 4a5edb57ddd4 skipped: already exists Copying blob 735956b91a18 skipped: already exists Copying blob f3b732ca7f91 skipped: already exists Copying blob 5ce792dcacff [--------------------] 0.0b/0.0b Copying config 65a80bbe96 [--------------------] 0.0b/5.4KiB Writing manifest to image destination","title":"Push image to Docker Hub"},{"location":"installmysql/","text":"Installing MySQL on Ubuntu 18.04 \u00b6 Instructions for Installing MySQL on Ubuntu 18.04 Instructions for Installing MySQL on Ubuntu 18.04 ============================== =========== MySQL is an open source database management system, usually installed as part of a web server. -domain/cach-cai-dat-va-cau-hinh-caddy-web-server-tren-vps-ubuntu-18-04.html) LAMP Stack (Linux, Apache, MySQL, PHP / Python / Perl) . It uses a relational database and SQL (Structured Query Language) to manage its data. Installing MySQL on Ubuntu is pretty straightforward: update your packages, then install the mysql-server package and run the included security command. sudo apt update sudo apt install mysql-server sudo mysql_secure_installation This article will guide you through installing MySQL version 5.7 on Ubuntu 18.04. H\u01b0\u1edbng d\u1eabn c\u00e0i \u0111\u1eb7t MySQL tr\u00ean Ubuntu 18.04 B\u01b0\u1edbc 1 \u2013 C\u00e0i \u0111\u1eb7t MySQL \u00b6 Tr\u00ean Ubuntu 18.04, only the latest version of MySQL is included in the APT package by default. At the time of writing, it's MySQL 5.7. To install it, update the packages on your server with apt : sudo apt update Then install the default package: sudo apt install mysql-server This command will install MySQL, but won't prompt you to set a password or make any other configuration changes. Because this makes your MySQL installation unsafe, we will address this in step 2. Step 2 \u2013 Configuring MySQL \u00b6 For fresh installations, you will need to run a security script. attached. This changes some of the less secure default options for things like remote root logins and sample users. On older versions of MySQL, you also need to initialize the data directory manually, but this is done automatically with the following. Run the security script: sudo mysql_secure_installation This will take you through a series of prompts, you can make some changes to the security options of your MySQL installation. The first prompt will ask if you want to install the Password Validation Plugin, which can be used to test your MySQL password strength. The next prompt will be to set a password for the root MySQL user. Enter and then confirm your secure password. You can press Y and then ENTER to accept the defaults for all subsequent questions. This will delete some anonymous users and test databases, disable remote root login. To initialize the MySQL data directory, you would use mysql_install_db for versions prior to 5.7.6 and mysqld \u2013initialize for 5.7.6 and later. Step 3 \u2013 (Optional) Adjusting User Authentication and Permissions \u00b6 In Ubuntu systems running MySQL 5.7 (and later), the MySQL user root is set to authenticate by use auth_socket plugin by default instead of with password. This allows for some greater security and usability in many cases, but it can also complicate things when you need to allow an external program (e.g. phpMyAdmin). To use a password to connect to MySQL as root , you will need to switch its authentication method from auth_socket to mysql_native_password . To do this, open MySQL from your terminal: sudo mysql Next, check what authentication method each of your MySQL user accounts uses with the following command: SELECT user,authentication_string,plugin ,host FROM mysql.user; +---------+--------------------- ------------------------------------------------------------+--- --------+ | user | authentication_string | plugin | host | +---------------------------------------------- -------------+-----------------------+-------+ | root | | auth_socket | localhost | | mysql.session | *THISISNOTAVALIDPASSWORDTHATCANBEUSEDHERE | mysql_native_password | localhost | | mysql.sys | *THISISNOTAVALIDPASSWORDTHATCANBEUSEDHERE | mysql_native_password | localhost | | debian-sys-maint | *FE8A77A22D490EAEDE9F4402E318F023C476EAD8 | mysql_native_password | localhost | +---------------------------------------------- -------------+-----------------------+-------+ To configure the root account for password authentication, run the following ALTER USER command. Be sure to change password to a strong password of your choice and note that this command will change the root password you set in Step 2: ALTER USER 'root'@'localhost' IDENTIFIED WITH mysql_native_password BY 'password '; Then run FLUSH PRIVILEGES; to tell the server to reload the tables and put your new changes into effect: FLUSH PRIVILEGES; Check the authentication methods used by each of your users again to confirm that root is no longer authenticating using the auth_socket plugin: SELECT user,authentication_string,plugin,host FROM mysql.user; +------- -----------+--------------------------------------------------------- -----+-----------------------+------+ | user | authentication_string | plugin | host | +---------------------------------------------- -------------+-----------------------+-------+ | root | *9DB839FF16B82668003976BF5BA0AB42F870745F | mysql_native_password | localhost | | mysql.session | *THISISNOTAVALIDPASSWORDTHATCANBEUSEDHERE | mysql_native_password | localhost | | mysql.sys | *THISISNOTAVALIDPASSWORDTHATCANBEUSEDHERE | mysql_native_password | localhost | | debian-sys-maint | *FE8A77A22D490EAEDE9F4402E318F023C476EAD8 | mysql_native_password | localhost | +---------------------------------------------- -------------+-----------------------+-------+ You can see in this example output that root user MySQL now authenticates with a password. Once you confirm this on your own server, you can exit the MySQL shell: exit To create a new user, open mysql again: sudo mysql Note: If you If you have enabled password authentication for root instead of using the auth_socket plugin, as described in the previous paragraphs you will need to use a different command to access the MySQL shell. The following will run your MySQL client with normal user privileges and you will only get administrator privileges in the database by authenticating: mysql -u root -p Enter password and Enter. Then, create a new user and give it a password: CREATE USER 'ntc'@'localhost' IDENTIFIED BY 'pass'; List it out: SELECT user,authentication_string,plugin,host FROM mysql .user; +-------------------+------------------------- -------------------------------------------------------------------------- ----+ | user | authentication_string | plugin | host | +---------------------------------------------- -------------+-----------------------+-------+ | root | *9DB839FF16B82668003976BF5BA0AB42F870745F | mysql_native_password | localhost | | mysql.session | *THISISNOTAVALIDPASSWORDTHATCANBEUSEDHERE | mysql_native_password | localhost | | mysql.sys | *THISISNOTAVALIDPASSWORDTHATCANBEUSEDHERE | mysql_native_password | localhost | | debian-sys-maint | *FE8A77A22D490EAEDE9F4402E318F023C476EAD8 | mysql_native_password | localhost | | ntc | *5403678D7E10A9C8DC490C129FAF5C10BF0F95EE | mysql_native_password | localhost | +---------------------------------------------- -------------+-----------------------+-------+ 5 rows in set (0.00 sec) Next is to grant your new user the appropriate privileges. For example, you can grant user privileges to all tables in the database, as well as add, edit, and remove user permissions, with this command: GRANT ALL PRIVILEGES ON *.* TO 'ntc '@'localhost' WITH GRANT OPTION; Note that, at this point, you do not need to run the FLUSH PRIVILEGES command again. This command is only needed when you modify level tables using statements such as INSERT , UPDATE or DELETE . Since you have already created a new user, instead of modifying an existing user, FLUSH PRIVILEGES is not needed. Exit mysql: exit; . Finally, check the MySQL settings. Step 4 \u2013 Testing MySQL \u00b6 Regardless of how you installed it, MySQL will start running automatically. To check this, check its status. systemctl status mysql.service If MySQL is not running, you can start it with sudo systemctl start mysql . Create a new database named wordpress with the following command: CREATE DATABASE wordpress DEFAULT CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci; Show databases: show databases; +-------------- ------+ | Database | +--------------------+ | information_schema | | mysql | | performance_schema | | sys | | wordpress | +--------------------+ 5 rows in set (0.00 sec) Allows a new user to be allowed to access this database. GRANT ALL PRIVILEGES ON database_name.* TO 'username'@'localhost'; or create a new user and give database access: GRANT ALL ON database_name.* TO 'username'@'localhost' IDENTIFIED BY 'password';","title":"Installing MySQL on Ubuntu 18.04"},{"location":"installmysql/#installing-mysql-on-ubuntu-1804","text":"Instructions for Installing MySQL on Ubuntu 18.04 Instructions for Installing MySQL on Ubuntu 18.04 ============================== =========== MySQL is an open source database management system, usually installed as part of a web server. -domain/cach-cai-dat-va-cau-hinh-caddy-web-server-tren-vps-ubuntu-18-04.html) LAMP Stack (Linux, Apache, MySQL, PHP / Python / Perl) . It uses a relational database and SQL (Structured Query Language) to manage its data. Installing MySQL on Ubuntu is pretty straightforward: update your packages, then install the mysql-server package and run the included security command. sudo apt update sudo apt install mysql-server sudo mysql_secure_installation This article will guide you through installing MySQL version 5.7 on Ubuntu 18.04. H\u01b0\u1edbng d\u1eabn c\u00e0i \u0111\u1eb7t MySQL tr\u00ean Ubuntu 18.04","title":"Installing MySQL on Ubuntu 18.04"},{"location":"installmysql/#buoc-1-cai-at-mysql","text":"Tr\u00ean Ubuntu 18.04, only the latest version of MySQL is included in the APT package by default. At the time of writing, it's MySQL 5.7. To install it, update the packages on your server with apt : sudo apt update Then install the default package: sudo apt install mysql-server This command will install MySQL, but won't prompt you to set a password or make any other configuration changes. Because this makes your MySQL installation unsafe, we will address this in step 2.","title":"B\u01b0\u1edbc 1 \u2013 C\u00e0i \u0111\u1eb7t MySQL"},{"location":"installmysql/#step-2-configuring-mysql","text":"For fresh installations, you will need to run a security script. attached. This changes some of the less secure default options for things like remote root logins and sample users. On older versions of MySQL, you also need to initialize the data directory manually, but this is done automatically with the following. Run the security script: sudo mysql_secure_installation This will take you through a series of prompts, you can make some changes to the security options of your MySQL installation. The first prompt will ask if you want to install the Password Validation Plugin, which can be used to test your MySQL password strength. The next prompt will be to set a password for the root MySQL user. Enter and then confirm your secure password. You can press Y and then ENTER to accept the defaults for all subsequent questions. This will delete some anonymous users and test databases, disable remote root login. To initialize the MySQL data directory, you would use mysql_install_db for versions prior to 5.7.6 and mysqld \u2013initialize for 5.7.6 and later.","title":"Step 2 \u2013 Configuring MySQL"},{"location":"installmysql/#step-3-optional-adjusting-user-authentication-and-permissions","text":"In Ubuntu systems running MySQL 5.7 (and later), the MySQL user root is set to authenticate by use auth_socket plugin by default instead of with password. This allows for some greater security and usability in many cases, but it can also complicate things when you need to allow an external program (e.g. phpMyAdmin). To use a password to connect to MySQL as root , you will need to switch its authentication method from auth_socket to mysql_native_password . To do this, open MySQL from your terminal: sudo mysql Next, check what authentication method each of your MySQL user accounts uses with the following command: SELECT user,authentication_string,plugin ,host FROM mysql.user; +---------+--------------------- ------------------------------------------------------------+--- --------+ | user | authentication_string | plugin | host | +---------------------------------------------- -------------+-----------------------+-------+ | root | | auth_socket | localhost | | mysql.session | *THISISNOTAVALIDPASSWORDTHATCANBEUSEDHERE | mysql_native_password | localhost | | mysql.sys | *THISISNOTAVALIDPASSWORDTHATCANBEUSEDHERE | mysql_native_password | localhost | | debian-sys-maint | *FE8A77A22D490EAEDE9F4402E318F023C476EAD8 | mysql_native_password | localhost | +---------------------------------------------- -------------+-----------------------+-------+ To configure the root account for password authentication, run the following ALTER USER command. Be sure to change password to a strong password of your choice and note that this command will change the root password you set in Step 2: ALTER USER 'root'@'localhost' IDENTIFIED WITH mysql_native_password BY 'password '; Then run FLUSH PRIVILEGES; to tell the server to reload the tables and put your new changes into effect: FLUSH PRIVILEGES; Check the authentication methods used by each of your users again to confirm that root is no longer authenticating using the auth_socket plugin: SELECT user,authentication_string,plugin,host FROM mysql.user; +------- -----------+--------------------------------------------------------- -----+-----------------------+------+ | user | authentication_string | plugin | host | +---------------------------------------------- -------------+-----------------------+-------+ | root | *9DB839FF16B82668003976BF5BA0AB42F870745F | mysql_native_password | localhost | | mysql.session | *THISISNOTAVALIDPASSWORDTHATCANBEUSEDHERE | mysql_native_password | localhost | | mysql.sys | *THISISNOTAVALIDPASSWORDTHATCANBEUSEDHERE | mysql_native_password | localhost | | debian-sys-maint | *FE8A77A22D490EAEDE9F4402E318F023C476EAD8 | mysql_native_password | localhost | +---------------------------------------------- -------------+-----------------------+-------+ You can see in this example output that root user MySQL now authenticates with a password. Once you confirm this on your own server, you can exit the MySQL shell: exit To create a new user, open mysql again: sudo mysql Note: If you If you have enabled password authentication for root instead of using the auth_socket plugin, as described in the previous paragraphs you will need to use a different command to access the MySQL shell. The following will run your MySQL client with normal user privileges and you will only get administrator privileges in the database by authenticating: mysql -u root -p Enter password and Enter. Then, create a new user and give it a password: CREATE USER 'ntc'@'localhost' IDENTIFIED BY 'pass'; List it out: SELECT user,authentication_string,plugin,host FROM mysql .user; +-------------------+------------------------- -------------------------------------------------------------------------- ----+ | user | authentication_string | plugin | host | +---------------------------------------------- -------------+-----------------------+-------+ | root | *9DB839FF16B82668003976BF5BA0AB42F870745F | mysql_native_password | localhost | | mysql.session | *THISISNOTAVALIDPASSWORDTHATCANBEUSEDHERE | mysql_native_password | localhost | | mysql.sys | *THISISNOTAVALIDPASSWORDTHATCANBEUSEDHERE | mysql_native_password | localhost | | debian-sys-maint | *FE8A77A22D490EAEDE9F4402E318F023C476EAD8 | mysql_native_password | localhost | | ntc | *5403678D7E10A9C8DC490C129FAF5C10BF0F95EE | mysql_native_password | localhost | +---------------------------------------------- -------------+-----------------------+-------+ 5 rows in set (0.00 sec) Next is to grant your new user the appropriate privileges. For example, you can grant user privileges to all tables in the database, as well as add, edit, and remove user permissions, with this command: GRANT ALL PRIVILEGES ON *.* TO 'ntc '@'localhost' WITH GRANT OPTION; Note that, at this point, you do not need to run the FLUSH PRIVILEGES command again. This command is only needed when you modify level tables using statements such as INSERT , UPDATE or DELETE . Since you have already created a new user, instead of modifying an existing user, FLUSH PRIVILEGES is not needed. Exit mysql: exit; . Finally, check the MySQL settings.","title":"Step 3 \u2013 (Optional) Adjusting User Authentication and Permissions"},{"location":"installmysql/#step-4-testing-mysql","text":"Regardless of how you installed it, MySQL will start running automatically. To check this, check its status. systemctl status mysql.service If MySQL is not running, you can start it with sudo systemctl start mysql . Create a new database named wordpress with the following command: CREATE DATABASE wordpress DEFAULT CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci; Show databases: show databases; +-------------- ------+ | Database | +--------------------+ | information_schema | | mysql | | performance_schema | | sys | | wordpress | +--------------------+ 5 rows in set (0.00 sec) Allows a new user to be allowed to access this database. GRANT ALL PRIVILEGES ON database_name.* TO 'username'@'localhost'; or create a new user and give database access: GRANT ALL ON database_name.* TO 'username'@'localhost' IDENTIFIED BY 'password';","title":"Step 4 \u2013 Testing MySQL"},{"location":"s3lifecycleextend/","text":"S3 Lifecycle Extend \u00b6 Reset S3 Object Timestamp for Bucket Lifecycle Expiration S3 buckets allow you to specify lifecycle rules that tell AWS to automatically delete or archive any objects in that bucket after a specific number of days. You can also specify a prefix with each rule so that different objects in the same bucket stay for different amounts of time. Example 1: I created a bucket named logs.example.com (not the real name) that automatically archives an object to AWS Glacier after it has been sitting in S3 for 90 days. Example 2: I created a bucket named tmp.example.com (not the real name) that automatically delete a file after it has been sitting there for 30 days. This works great until you realize that there are specific files that you want to keep around for just a bit longer than its original expiration. You could download and then upload the object to reset its creation date, thus starting the countdown from zero; but through a little experimentation, I found that one easy way to reset the creation/modification timestamp on an S3 object is to ask S3 to change the object storage method to the same storage method it currently has. The following example uses the new aws-cli command line tool to reset the timestamp of an S3 object, thus restarting the lifecycle counter. This has an effect similar to the Linux/Unix touch command. Setup \u00b6 Create a test S3 bucket and add a lifecycle to delete any object that is over 1 day old. bucket=tmp-test.alestic.com aws s3 mb s3://$bucket aws s3api put-bucket-lifecycle --bucket $bucket --lifecycle-configuration '{\"Rules\":[{\"Status\":\"Enabled\",\"Prefix\":\"\",\"Expiration\":{\"Days\":1}}]}' Upload a test file. file=somefile.txt aws s3 cp $file s3://$bucket/ Look at the timestamp on the test file in the test bucket. aws s3 ls s3://$bucket Wait a bit (seconds, hours, whatever). Reset S3 Object Timestamp \u00b6 Modify the timestamp on the file to be the current time, by telling Amazon to change the storage class to the current storage class. If you are storing objects with REDUCED-REDUNDANCY storage, then specify that instead of STANDARD . aws s3api copy-object --storage-class STANDARD --copy-source $bucket/$file --bucket $bucket --key $file Look at the timestamp again, to make sure it has been updated. aws s3 ls s3://$bucket The response JSON from the copy-object command will tell you the new expiration date. Cleanup \u00b6 If you followed the above commands to create a temporary test file and bucket, you can delete the file by waiting a couple days for it to expire, or run this: aws s3 rm s3://$bucket/$file Delete the (empty) bucket using: aws s3 rb s3://$bucket","title":"S3 Lifecycle Extend"},{"location":"s3lifecycleextend/#s3-lifecycle-extend","text":"Reset S3 Object Timestamp for Bucket Lifecycle Expiration S3 buckets allow you to specify lifecycle rules that tell AWS to automatically delete or archive any objects in that bucket after a specific number of days. You can also specify a prefix with each rule so that different objects in the same bucket stay for different amounts of time. Example 1: I created a bucket named logs.example.com (not the real name) that automatically archives an object to AWS Glacier after it has been sitting in S3 for 90 days. Example 2: I created a bucket named tmp.example.com (not the real name) that automatically delete a file after it has been sitting there for 30 days. This works great until you realize that there are specific files that you want to keep around for just a bit longer than its original expiration. You could download and then upload the object to reset its creation date, thus starting the countdown from zero; but through a little experimentation, I found that one easy way to reset the creation/modification timestamp on an S3 object is to ask S3 to change the object storage method to the same storage method it currently has. The following example uses the new aws-cli command line tool to reset the timestamp of an S3 object, thus restarting the lifecycle counter. This has an effect similar to the Linux/Unix touch command.","title":"S3 Lifecycle Extend"},{"location":"s3lifecycleextend/#setup","text":"Create a test S3 bucket and add a lifecycle to delete any object that is over 1 day old. bucket=tmp-test.alestic.com aws s3 mb s3://$bucket aws s3api put-bucket-lifecycle --bucket $bucket --lifecycle-configuration '{\"Rules\":[{\"Status\":\"Enabled\",\"Prefix\":\"\",\"Expiration\":{\"Days\":1}}]}' Upload a test file. file=somefile.txt aws s3 cp $file s3://$bucket/ Look at the timestamp on the test file in the test bucket. aws s3 ls s3://$bucket Wait a bit (seconds, hours, whatever).","title":"Setup"},{"location":"s3lifecycleextend/#reset-s3-object-timestamp","text":"Modify the timestamp on the file to be the current time, by telling Amazon to change the storage class to the current storage class. If you are storing objects with REDUCED-REDUNDANCY storage, then specify that instead of STANDARD . aws s3api copy-object --storage-class STANDARD --copy-source $bucket/$file --bucket $bucket --key $file Look at the timestamp again, to make sure it has been updated. aws s3 ls s3://$bucket The response JSON from the copy-object command will tell you the new expiration date.","title":"Reset S3 Object Timestamp"},{"location":"s3lifecycleextend/#cleanup","text":"If you followed the above commands to create a temporary test file and bucket, you can delete the file by waiting a couple days for it to expire, or run this: aws s3 rm s3://$bucket/$file Delete the (empty) bucket using: aws s3 rb s3://$bucket","title":"Cleanup"}]}